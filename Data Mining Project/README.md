## Data

The two primary data sets for the project were housed in the user_info.csv and user_log.csv file. User_info.csv contained a list
of 212,062 distinct anonymized user id’s paired with gender and age range.  User_log.csv contained a list of 26,258,292 
transactions between user’s found in the user_info.csv file and merchants. These transactions are characterized by 4 
action types: Clicks, Add-To-Cart, Purchased, and Add-To-Favorite, designated by the numbers, 0, 1, 2, and 3 respectively. 
23,151,754 transactions were clicks, 34,004 add-to-cart, 1,643,970 purchased, and 1,428,564 add-to-favorite. 
Along with these two datasets a train and test file were provided. The train.csv file contained a subset of the population 
paired with merchant id’s and labels 0 or 1 indicating whether or not a customer would purchase an Item from a specified
merchant in the next 6 months. The Test file contained another sampling of the user population with an empty column in which to 
place probabilities representing the likely-hood that a customer would purchase an Item in the next 6 months. 

## Preparing cleaning Data

An oracle Database set up on AWS as an RDS instance with 100 gigabytes of space was used to Explore and clean the data. The 
user_info.csv file was imported into a table with columns corresponding to the fields separating records in the original 
user_info file along with a primary key constraint on the user_id column designating the anonymized user’s identification number.
The user_log.csv file was imported into a table without any primary key constraint.  Both the train.csv and test.csv files were 
loaded into tables after removing the ‘#’ symbol separating user_id and merchant_id, then replacing the symbol with a ‘,’.  The 
train table consisted of three columns: user_id, merchant_id, and prob with a primary key constraint on user_id and merchant_id. 
The Test table consisted of two columns: user_id and merchant_id, also created with a primary key constraint on user_id, and 
merchant_id. The data type for each field of the four tables was designated as integer upon creation of the tables. 

The data base was queried with a combination of aggregate functions, and group by queries to provide counts, sums, averages, etc.
of various combinations of the columns. A summary of the information obtained by these methods may be found in data_info.txt 
file accompanying this report.  After some analysis of the data in this fashion, it was decided that an aggregation of the action
types for each customer along with the customer’s gender and age range generated a suitable feature set for classification.

Features were generated by creating and populating tables from these initial datasets (Process described in the next section). 
After composition on the database level, the feature set was standardized programmatically using the StandardScaler function from
the sklearn.preprocessing library before classification, calling the function fit_transform on the train and test feature sets.

## Feature Generation

The feature set used for classification was generated by first aggregating the total counts of each action type for each 
customer/merchant pair documented in the user_log table. The result of this query on the user_log was inserted into a new table, 
user_merchant_action, composed of the columns user_id, merchant_id, total_click, total_add_to_cart, total_purchase, and 
total_add_to_favorite with a primary key constraint on the user_id and merchant_id column (initialized with 0 values before 
update). The update queries used to store these values in the user_merchant_action along with all other SQL statements that were
run for feature generation can be found in the document entitled Queries.txt. 

Once the aggregated data for each customer, merchant pair was generated and stored in the user_merchant_action table, two 
additional tables: user_merchant_action_test_label, and user_merchant_action_train_label, were created to store a natural join
of information from the user_merchant_action table, and user_info table on the train and test tables. 
User_merchant_action_train_label contained the columns: user_id, merchant_id, age_range, gender, total_click, total_add_to_cart,
total_purchase, total_add_to_favorite, and prob with a primary key constraint on user_id and merchant_id. The table 
User_merchant_action_test_label consisted of like fields without the prob column and the same primary key constraint. Once 
created and populated with relevant data, the tables User_merchant_action_test_label and User_merchant_action_train_label were 
exported to csv files to be used for classification after standardization. Each of the datasets generated in this fashion are 
included in the submission file. The scripts written to classify the data are set to run with this data. 

## Models
Once features were generated using an oracle database, the job of classification was passed to python scripts employing various
functions from the scikit.learn library. Each function used for classification is commented out in the script accompanying this
document. All one need due to run any given test is comment the code and edit the parameters as desired. The functions 
test_KNN_Acc and test_RF_Acc were used to iteratively test the success of each classifier between various ranges of parameters.
These are defined in the Data class housing all the functions used for classification and testing, but not commented out in the
code. 

## Successful Models

All tests were performed using the script project.py. Project.py Contains a class, “Data”, which reads in the contents of the 
each of the feature sets described in the feature generation section. Upon construction of a data object, the user_id, and 
merchant_id columns are separated from the feature data for testing. Additionally, the Data class contains functions to perform
classification of the feature sets with different models, test different models with a range of parameters, standardize data, 
and recombine the results of tests with the original merchant customer pairs and classification results into a csv file. 

The most successful model for feature generation tested was the scikitlearn SVC imported from the sklearn.svm library. The SVC 
scored a .22922 when evaluated using Kaggle’s log loss scoring system.  SVC consistently outperformed the other models tested. 
The results obtained by running the scikitlearn libraries SVC can be reproduced by a call to the Class_SVM function on a Data 
object in the project.py script. 

Considering a strict assignment of 0 or 1 to each customer/merchant pair on the training data, with 0 designating a customer 
who will not purchase a product in the next six months and 1 a customer who will, the SVM tends to err more towards false 
negatives than false positives.

## Project Summary

In the beginning of the project a great deal of attention was placed on feature generation. One of the wrong turns I made in 
the beginning was to attempt clustering on the substantial dataset provided. This did not prove very useful without cleaning 
and aggregating the data. Also, far too much time was spent examining the relative counts of varying sub populations of 
customers, focusing on edge cases and examine customers by the items they buy rather than buy their general buying behavior. 
Focusing on the relative frequency of each customer’s aggregate action type for each merchant proved to be a sufficient 
predictor for future behavior. For instance, if a customer has a relatively low click count with a merchant but a high 
purchase count, they may appear to be a promising lead, but in reality, show more impulse than loyalty to a given merchant. 
Simply buying on impulse tends to be a less promising predictor of customer loyalty than those who exhibit a high click count 
and a somewhat moderate purchase count. Taken as a ratio, clicks to purchases reveal more about a merchant’s future prospects 
with a customer than mere purchasing patterns. Counter intuitive relationships such as this reveal the importance of dropping 
assumptions about the data and letting the relationships found by aggregation of the data elements speak for themselves. 

Another lesson learned through numerous trials of different learning algorithms was the value of a confusion matrix over 
scoring metrics for accuracy. For example, the cost of a false positive can greatly outweigh gains in overall accuracy when 
there are relatively few positive results in a large dataset. If the majority of a sample of customers prove to be infrequent 
in the future, a classification algorithm that favors this group can appear to be accurate when examined by a simple hit or 
miss ration but behave unfavorably when classifying frequent customers is the primary aim of the model. This tradeoff can be 
seen in the accuracy reported by the random forest classifier and the SVC classifier. While Random Forest reports an accuracy 
of around 94 % for any given trial, and SVC only 93 % the difference in false positives makes SVC an overwhelmingly better 
choice.

Initially I chose to use Matlab for classification. I operated under the assumption that Matlab would be better suited for 
matrix operation on the data. Although this may be true, Matlab did not prove to be quite as adept as the scikit.learn library 
at actual classification tasks. Using Matlab’s version of SVC with a probability parameter passed to produce probabilities of 
the positive class proved to be too much for Matlab to handle. Computations on the large train and test data set took hours, 
and did not produce results sufficient for submission.

