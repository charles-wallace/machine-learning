# Linear Classifiers

## 3.1
**a.** *Generate two data sets X1 and X1' of N = 200 two-dimensional vectors
each. The first half of the vectors stem from the normal distribution with
m1 = [5, 0]<sup>T</sup> and S1 = I , while the second half of the vectors stem
from the normal distribution with m1 = [5, 0]<sup>T</sup> and S1 = I , where I is the
identity 2x2 matrix. Append each vector of both X1 and X1' by inserting
an additional coordinate, which is set equal to 1.*
  
**Solution:** The code used to complete 3.1 may be found in the comp_exp3_1.m matlab file.
  
**b.** *Apply the perceptron algorithm, the sum of error squares classifier, and the
LMS algorithm on the previous data set, using various initial values for the
parameter vector (where necessary)*
  
**Solution:**

**Perceptron**
With the Perceptron initialized to the initial value vector [.5,.5,.5]’ and rho set to .5 the following figure   was produced for the Gaussian classes generated by the means [-5,0]’ and [5,0] with the covariance matrix initialized to the  
identity matrix.

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_1b.png?raw=true" height="200" />

**Sum Of Error Squares Classifier**
Using the same values for each respective classes’ mean and covariance the following decision line was generated in comp_exp_3_1.m

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_1b_2.png" height="200" />

**SVM**
For the SVM, the parameter C was initialized to 0 due to the fact that the classes are linearly separable. Using the same values for each respective classes’ mean and covariance the following decision line was generated in comp_exp_3_1.m

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_1b_3.png" height="200" />

## 3.2
*Repeat experiment 1 using now the sets X2 and X2' whose first half of their
vectors stem from the normal distribution with m1 = [-2,0]<sup>T</sup> and S1 = I ,
while the second half of their vectors stem from the normal distribution with
m1 = [2,0]<sup>T</sup> and S1 = I.*

**Solution:** The following figures were produced for the Gaussian classes generated by the means [-2,0]’ and [2,0]’ and covariance matrices initialized to the identity matrix.

**Perceptron**
Pictured perceptron line was initialized with the initial value vector [.5,.5,.5]’ and rho set to .5 

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_2_perceptron.png" height="200" />

**Sum of Error Squares Classifier**
Using the same values for each respective classes’ mean and covariance the following decision line was generated in comp_exp_3_2.m

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_2_Sum_Squares.png" height="200" />

**SVM**
For the SVM, the parameter C was initialized to .1 . Using the same values for each respective classes’ mean and covariance the following decision line was generated in comp_exp_3_2.m

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_2_SVM.png" height="200" />

**The Following Describes the Classifiers’ Error:**
<table style="width:100%">
<caption><b>Perceptron Classifier</b></caption>
<tr>
<th>% Error</th>
<th>w_ini</th>
<th>rho</th>
</tr>
<tr> <td>3.50%</td> <td>[0.1,0.1,0.1]'</td> <td>0.4</td> </tr>
<tr> <td>3.50%</td> <td>[0.1,0.1,0.1]'</td> <td>0.5</td> </tr>
<tr> <td>3.50%</td> <td>[0.5,0.5,0.5]'</td> <td>0.5</td> </tr>
<tr> <td>6.00%</td> <td>[1, 1, 1 ]'</td> <td>0.5</td> </tr>
</table>

<table style="width:100%">
<caption><b>Sum of Square Err. Classifier</b></caption>
<tr>
<th>% Error</th>
</tr>
<tr> <td>4.00%</td> </tr>
</table>

<table style="width:100%">
<caption><b>SVM Classifier</b></caption>
<tr>
<th>% Error</th>
<th>C</th>
</tr>
<tr> <td>4.00%</td> <td>0.5</td> </tr>
<tr> <td>3.50%</td> <td>10</td> </tr>
<tr> <td>3.50%</td> <td>12</td> </tr>
<tr> <td>3.50%</td> <td>15</td> </tr>
</table>

## 3.3
*Repeat experiment 3.1 using now the sets X3 and X3' whose first half of the
vectors stem from the normal distribution with m1 = [-1, 0]<sup>T</sup> and S1 = I ,
while the second half of the vectors stem from the normal distribution with
m1 = [1, 0]<sup>T</sup> and S1 = I .*

**Solution:** The code used to complete 3.3 may be found in the comp_exp3_3.m matlab file.

**Perceptron**
With the Perceptron initialized to the initial value vector [.5,.5,.5]’ and rho set to .5 the following figure was produced. for the Gaussian classes generated by the means [-1,0]’ and [1,0]’ with the covariance matrix initialized to the identity matrix.

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_3_Perceptron.png" height="200" />

**Sum of Error Squares Classifier**
Using the same values for each respective classes mean and covariance the following decision line was generated in comp_exp_3_3.m

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_3_Sum_Square.png" height="200" />

**SVM**
For the SVM, the parameter C was initialized to .02. The following decision line was generated in comp_exp_3_2.m

<img src="https://github.com/cyw214/Machine-Learning/blob/master/3_3_SVM.png" height="200" />

**The Following Describes the Classifiers’ Error:**
<table style="width:100%">
<caption><b>Perceptron Classifier</b></caption>
<tr>
<th>% Error</th>
<th>w_ini</th>
<th>rho</th>
</tr>
<tr> <td>20.50%</td> <td>[0.1,0.1,0.1]'</td> <td>0.4</td> </tr>
<tr> <td>19.00%</td> <td>[0.1,0.1,0.1]'</td> <td>0.5</td> </tr>
<tr> <td>17.50%</td> <td>[0.5,0.5,0.5]'</td> <td>0.5</td> </tr>
<tr> <td>20.00%</td> <td>[1, 1, 1 ]'</td> <td>0.5</td> </tr>
</table>

<table style="width:100%">
<caption><b>Sum of Square Err. Classifier</b></caption>
<tr>
<th>% Error</th>
</tr>
<tr> <td>19.00%</td> </tr>
</table>

<table style="width:100%">
<caption><b>SVM Classifier</b></caption>
<tr>
<th>% Error</th>
<th>C</th>
</tr>
<tr> <td>20.50%</td> <td>0.01</td> </tr>
<tr> <td>20.50%</td> <td>0.03</td> </tr>
<tr> <td>20.00%</td> <td>70</td> </tr>
<tr> <td>18.50%</td> <td>90</td> </tr>
<tr> <td>19.00%</td> <td>100</td> </tr>
<tr> <td>20.50%</td> <td>200</td> </tr>
</table>

## 3.4
**Discussion**
In general, the perceptron produced the most accurate classification for both separable and inseparable data. The option to set a point for the the perceptron’s decent, as well as the option to set rho allows for finer tuning. This can be exploited to improve accuracy of the perceptron’s resulting decision line.  In 3.3 the lowest observed classification error for the perceptron was 17.50%, while the SVM only achieved as low as 18.50%. Within the context of problems 3.1 to 3.3, empirical evidence suggests that the perceptron’s optimal performance beats Optimal performance of the SVM. That being said, taking into account the results from 3.2, The SVM proves to be more reliable across a wider range of initial values for the parameter C. Its performance scales by constraints on the margin by C. Larger values of C mean tighter margins, resulting in better estimations of class separation. The error in the SVM’s classification of data upon initialization with smaller values of C is limited by the margin width’s span of the data. When some initial value of C widens the Margin width to a distance greater than the interval on which a set of data occurs, all that is left to classify unknown feature vectors is the hyper plane centered between the margins. As the Margins are expanded beyond the interval of the training set, the minimizing function of the margins no longer have any impact on the placement of the hyperplane,  positioning the hyper place in the same position after continued expansion of the margins.  C can also be too big. The closer the margins are to the decision line in the middle of the margins, the closer it gets to the same problem created by making the margins too small. The Sum of Err. Square Classifier is very Straightforward. It takes no parameters other than a training set and the training set’s class vector. No tuning is possible. Given the same data set, it produces the same classification error every time.
